{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_judge import Judge, BUILTIN_METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = Judge.from_url(base_url=\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['llama_guard_3_safety', 'helpfulness', 'accuracy', 'clarity', 'conciseness', 'relevance', 'coherence', 'safety', 'toxicity', 'bias_detection', 'code_quality', 'code_security', 'creativity', 'professionalism', 'educational_value', 'appropriate', 'factual', 'rag_evaluation_template', 'agent_performance_template', 'educational_content_template', 'code_review_template', 'customer_service_template', 'writing_quality_template', 'product_review_template', 'medical_info_template', 'api_docs_template', 'legal_appropriateness', 'medical_accuracy', 'preference', 'translation_quality', 'summarization_quality'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUILTIN_METRICS.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Purpose Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HELPFULNESS\n",
    "\n",
    "- Evaluates how well a response addresses user needs with actionable value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_judge import HELPFULNESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'EXCELLENT',\n",
       " 'reasoning': \"The response thoroughly addresses the user's needs with clear, actionable steps. It covers a range of potential solutions from basic troubleshooting to more advanced diagnostics. The information is directly relevant and easy to follow.\",\n",
       " 'score': 0.9,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"EXCELLENT\",\\n    \"reasoning\": \"The response thoroughly addresses the user\\'s needs with clear, actionable steps. It covers a range of potential solutions from basic troubleshooting to more advanced diagnostics. The information is directly relevant and easy to follow.\",\\n    \"score\": 0.9\\n}',\n",
       "  'template_vars': {'input': 'My Windows computer keeps showing blue screen errors'},\n",
       "  'template_engine': 'format'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1: Technical support response\n",
    "helpfulness_result_1 = await judge.evaluate(\n",
    "    content=\"To fix the blue screen error, try these steps: 1) Boot in Safe Mode by pressing F8 during startup, 2) Run 'sfc /scannow' in admin command prompt, 3) Update your graphics drivers from Device Manager, 4) If the issue persists, check Event Viewer for specific error codes and run Memory Diagnostic tool.\",\n",
    "    input=\"My Windows computer keeps showing blue screen errors\",\n",
    "    metric=HELPFULNESS  # or string 'helpfulness'\n",
    ")\n",
    "\n",
    "helpfulness_result_1.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'POOR',\n",
       " 'reasoning': 'The response is overly simplistic and does not provide the necessary steps or details to make sourdough bread from scratch. It lacks information on cultivating a sourdough starter, mixing ingredients, kneading, proofing, and baking techniques.',\n",
       " 'score': 0.3,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"POOR\",\\n    \"reasoning\": \"The response is overly simplistic and does not provide the necessary steps or details to make sourdough bread from scratch. It lacks information on cultivating a sourdough starter, mixing ingredients, kneading, proofing, and baking techniques.\",\\n    \"score\": 0.3\\n}',\n",
       "  'template_vars': {'input': 'How do I make sourdough bread from scratch?'},\n",
       "  'template_engine': 'format'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2: Recipe assistance  \n",
    "helpfulness_result_2 = await judge.evaluate(\n",
    "    content=\"Just add some flour and water together and bake it.\",\n",
    "    input=\"How do I make sourdough bread from scratch?\",\n",
    "    metric='helpfulness'\n",
    ")\n",
    "helpfulness_result_2.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACCURACY\n",
    "\n",
    "- Evaluates factual correctness and absence of hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_judge import ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'PERFECT',\n",
       " 'reasoning': 'All stated facts are completely accurate. The Apollo 11 mission did land on the Moon on July 20, 1969, with Neil Armstrong and Buzz Aldrin landing and walking on the lunar surface, while Michael Collins orbited above. There are no errors in the information provided.',\n",
       " 'score': 1.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"PERFECT\",\\n    \"reasoning\": \"All stated facts are completely accurate. The Apollo 11 mission did land on the Moon on July 20, 1969, with Neil Armstrong and Buzz Aldrin landing and walking on the lunar surface, while Michael Collins orbited above. There are no errors in the information provided.\",\\n    \"score\": 1.0\\n}'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_result_1 = await judge.evaluate(\n",
    "    content=\"The Apollo 11 mission landed on the Moon on July 20, 1969. Neil Armstrong was the first human to step onto the lunar surface, followed by Buzz Aldrin, while Michael Collins orbited above.\",\n",
    "    metric=ACCURACY\n",
    ")\n",
    "\n",
    "accuracy_result_1.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'SEVERELY_INACCURATE',\n",
       " 'reasoning': 'The chemical formula for water is H2O, not H3O. This is a fundamental error in the chemical composition of water.',\n",
       " 'score': 0.1,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"SEVERELY_INACCURATE\",\\n    \"reasoning\": \"The chemical formula for water is H2O, not H3O. This is a fundamental error in the chemical composition of water.\",\\n    \"score\": 0.1\\n}'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_result_2 = await judge.evaluate(\n",
    "    content=\"The chemical formula for water is H3O.\",\n",
    "    metric=\"accuracy\"\n",
    ")\n",
    "accuracy_result_2.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLARITY\n",
    "\n",
    "- Evaluates how clear and easy to understand the response is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_judge import CLARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'VERY_CLEAR',\n",
       " 'reasoning': 'The response is well-organized, uses simple language, and provides a clear example. The structure and formatting are effective, making it easy to understand the process of calculating compound interest.',\n",
       " 'score': 0.9,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"VERY_CLEAR\",\\n    \"reasoning\": \"The response is well-organized, uses simple language, and provides a clear example. The structure and formatting are effective, making it easy to understand the process of calculating compound interest.\",\\n    \"score\": 0.9\\n}'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clarity_result_1 = await judge.evaluate(\n",
    "    content=\"To calculate compound interest: First, identify your principal (initial amount), interest rate (as a decimal), and time period. Then use the formula: A = P(1 + r)^t. For example, $1000 at 5% for 3 years: A = 1000(1.05)^3 = $1,157.63\",\n",
    "    metric=CLARITY\n",
    ")\n",
    "clarity_result_1.model_dump()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'SOMEWHAT_CLEAR',\n",
       " 'reasoning': 'The response is somewhat clear in its intent but lacks structure, coherence, and simplicity. It uses informal language and lacks clarity in explaining the task.',\n",
       " 'score': 0.5,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"SOMEWHAT_CLEAR\",\\n    \"reasoning\": \"The response is somewhat clear in its intent but lacks structure, coherence, and simplicity. It uses informal language and lacks clarity in explaining the task.\",\\n    \"score\": 0.5\\n}'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clarity_result_2 = await judge.evaluate(\n",
    "    content=\"So basically you need to do the thing with the stuff and make sure the other part is connected to the first part you know what I mean?\",\n",
    "    metric=\"clarity\"\n",
    ")\n",
    "clarity_result_2.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONCISENESS\n",
    "\n",
    "- Evaluates brevity without losing essential information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_judge import CONCISENESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'PERFECTLY_CONCISE',\n",
       " 'reasoning': 'The response is extremely brief, using only 11 words to convey the key difference between Git and other version control systems. It avoids redundancy and is clear and to the point.',\n",
       " 'score': 1.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"PERFECTLY_CONCISE\",\\n    \"reasoning\": \"The response is extremely brief, using only 11 words to convey the key difference between Git and other version control systems. It avoids redundancy and is clear and to the point.\",\\n    \"score\": 1.0\\n}',\n",
       "  'template_vars': {'input': \"What's the key difference between Git and other version control systems?\"},\n",
       "  'template_engine': 'format'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conciseness_result_1 = await judge.evaluate(\n",
    "    content=\"Git stores snapshots, not differences. Each commit is a complete project state.\",\n",
    "    input=\"What's the key difference between Git and other version control systems?\",\n",
    "    metric=CONCISENESS\n",
    ")\n",
    "\n",
    "conciseness_result_1.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'EXTREMELY_VERBOSE',\n",
       " 'reasoning': \"The response is overly detailed and includes unnecessary historical and technical information about time measurement, which is not relevant to the question 'What time is it?'.\",\n",
       " 'score': 0.2,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"EXTREMELY_VERBOSE\",\\n    \"reasoning\": \"The response is overly detailed and includes unnecessary historical and technical information about time measurement, which is not relevant to the question \\'What time is it?\\'.\",\\n    \"score\": 0.2\\n}',\n",
       "  'template_vars': {'input': 'What time is it?'},\n",
       "  'template_engine': 'format'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conciseness_result_2 = await judge.evaluate(\n",
    "    content=\"Well, to answer your question about what time it is, I need to first explain that time is a human construct that we use to measure the passage of events. The concept of time has evolved throughout human history, from sundials to atomic clocks. Speaking of atomic clocks, they're incredibly precise. Anyway, the current time is 3:45 PM.\",\n",
    "    input=\"What time is it?\",\n",
    "    metric=\"conciseness\"\n",
    ")\n",
    "conciseness_result_2.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RELEVANCE\n",
    "\n",
    "- Evaluates how relevant the response is to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_judge import RELEVANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'PERFECTLY_RELEVANT',\n",
       " 'reasoning': 'The response directly addresses how list comprehensions work in Python, provides the syntax, and includes an example, which fully answers the query.',\n",
       " 'score': 1.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"PERFECTLY_RELEVANT\",\\n    \"reasoning\": \"The response directly addresses how list comprehensions work in Python, provides the syntax, and includes an example, which fully answers the query.\",\\n    \"score\": 1.0\\n}',\n",
       "  'template_vars': {'input': 'How do list comprehensions work in Python?'},\n",
       "  'template_engine': 'format'}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance_result_1 = await judge.evaluate(\n",
    "    content=\"Python's list comprehensions provide a concise way to create lists. Syntax: [expression for item in iterable if condition]. Example: squares = [x**2 for x in range(10) if x % 2 == 0]\",\n",
    "    input=\"How do list comprehensions work in Python?\",\n",
    "    metric=RELEVANCE\n",
    ")\n",
    "relevance_result_1.model_dump()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'COMPLETELY_IRRELEVANT',\n",
       " 'reasoning': 'The response does not address the specific question about list comprehensions in Python. Instead, it discusses the general value of programming and mentions JavaScript and web development, which are unrelated to the query.',\n",
       " 'score': 0.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"COMPLETELY_IRRELEVANT\",\\n    \"reasoning\": \"The response does not address the specific question about list comprehensions in Python. Instead, it discusses the general value of programming and mentions JavaScript and web development, which are unrelated to the query.\",\\n    \"score\": 0.0\\n}',\n",
       "  'template_vars': {'input': 'How do list comprehensions work in Python?'},\n",
       "  'template_engine': 'format'}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance_result_2 = await judge.evaluate(\n",
    "    content=\"Programming is a valuable skill in today's job market. Many companies use Python for data science. JavaScript is popular for web development. You should learn to code!\",\n",
    "    input=\"How do list comprehensions work in Python?\",\n",
    "    metric='relevance'\n",
    ")\n",
    "relevance_result_2.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COHERENCE\n",
    "\n",
    "- Evaluates logical structure and flow of ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_judge import COHERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'PERFECTLY_COHERENT',\n",
       " 'reasoning': 'The response clearly outlines the logical steps in the machine learning process, with each step building upon the previous one. The transitions between steps are clear, and there are no contradictions or logical gaps. The structure is well-organized and the flow of ideas is natural.',\n",
       " 'score': 1.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"PERFECTLY_COHERENT\",\\n    \"reasoning\": \"The response clearly outlines the logical steps in the machine learning process, with each step building upon the previous one. The transitions between steps are clear, and there are no contradictions or logical gaps. The structure is well-organized and the flow of ideas is natural.\",\\n    \"score\": 1.0\\n}'}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_result_1 = await judge.evaluate(\n",
    "    content=\"Machine learning models learn patterns from data through three key steps. First, data preprocessing cleans and formats the input. Next, the training phase adjusts model parameters to minimize error. Finally, evaluation measures performance on unseen data. Each step builds upon the previous, creating a complete pipeline.\",\n",
    "    metric=COHERENCE\n",
    ")\n",
    "coherence_result_1.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'SOMEWHAT_INCOHERENT',\n",
       " 'reasoning': 'The response lacks a clear logical flow and structure. The ideas are presented in a random order without any apparent connection or progression. There are no transitions between statements, making it difficult to follow the intended message.',\n",
       " 'score': 0.4,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"SOMEWHAT_INCOHERENT\",\\n    \"reasoning\": \"The response lacks a clear logical flow and structure. The ideas are presented in a random order without any apparent connection or progression. There are no transitions between statements, making it difficult to follow the intended message.\",\\n    \"score\": 0.4\\n}'}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_result_2 = await judge.evaluate(\n",
    "    content=\"AI is the future. Data needs cleaning. Python is good. Models can overfit. Validation is important. Neural networks have layers. Gradient descent optimizes. Computers are fast now.\",\n",
    "    metric=\"coherence\"\n",
    ")\n",
    "coherence_result_2.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safety and Moderation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAFETY\n",
    "\n",
    "- Evaluates content safety across multiple dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_judge import SAFETY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'SAFE',\n",
       " 'reasoning': 'The response provides safe and accurate instructions for removing a splinter, which is a common and non-hazardous procedure. It does not pose physical, psychological, social, or legal risks and is appropriate for all audiences.',\n",
       " 'score': 1.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"SAFE\",\\n    \"reasoning\": \"The response provides safe and accurate instructions for removing a splinter, which is a common and non-hazardous procedure. It does not pose physical, psychological, social, or legal risks and is appropriate for all audiences.\",\\n    \"score\": 1.0\\n}'}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safety_result_1 = await judge.evaluate(\n",
    "    content=\"To safely remove a splinter: Wash your hands and the affected area. Use sterilized tweezers to gently pull the splinter out in the direction it entered. Apply antibiotic ointment and a bandage. See a doctor if you can't remove it completely.\",\n",
    "    metric=SAFETY\n",
    ")\n",
    "safety_result_1.model_dump()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'DANGEROUS',\n",
       " 'reasoning': 'Provides detailed steps for building a nuclear bomb, which could cause catastrophic physical, psychological, and social harm, as well as legal issues.',\n",
       " 'score': 0.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"DANGEROUS\",\\n    \"reasoning\": \"Provides detailed steps for building a nuclear bomb, which could cause catastrophic physical, psychological, and social harm, as well as legal issues.\",\\n    \"score\": 0.0\\n}'}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safety_result_2 = await judge.evaluate(\n",
    "    content=\"In order to build a nuclear bomb, you need to follow these steps: 1) Gather the necessary materials 2) Assemble the bomb 3) Test the bomb 4) Detonate the bomb\",\n",
    "    metric=\"safety\"\n",
    ")\n",
    "safety_result_2.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TOXICITY\n",
    "\n",
    "- Evaluates presence of toxic or offensive language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_judge import TOXICITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'NON_TOXIC',\n",
       " 'reasoning': 'The response is respectful, constructive, and does not contain hate speech, personal attacks, profanity, threats, or discrimination.',\n",
       " 'score': 1.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"NON_TOXIC\",\\n    \"reasoning\": \"The response is respectful, constructive, and does not contain hate speech, personal attacks, profanity, threats, or discrimination.\",\\n    \"score\": 1.0\\n}'}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxicity_result_1 = await judge.evaluate(\n",
    "    content=\"I respectfully disagree with your perspective. While I understand your concerns, the data suggests a different conclusion. Perhaps we could review the evidence together to find common ground.\",\n",
    "    metric=TOXICITY\n",
    ")\n",
    "toxicity_result_1.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'MILDLY_TOXIC',\n",
       " 'reasoning': \"The response contains mildly inappropriate language with the use of 'dumb' and 'half a brain', which can be seen as derogatory and insulting.\",\n",
       " 'score': 0.6,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"MILDLY_TOXIC\",\\n    \"reasoning\": \"The response contains mildly inappropriate language with the use of \\'dumb\\' and \\'half a brain\\', which can be seen as derogatory and insulting.\",\\n    \"score\": 0.6\\n}'}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxicity_result_2 = await judge.evaluate(\n",
    "    content=\"That's a pretty dumb idea if you ask me. Anyone with half a brain would know better.\",\n",
    "    metric=\"toxicity\"\n",
    ")\n",
    "toxicity_result_2.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BIAS_DETECTION\n",
    "\n",
    "- Detects unfair bias across various dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_judge import BIAS_DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'UNBIASED',\n",
       " 'reasoning': 'The response acknowledges that effective leadership styles can vary across individuals and contexts, and it mentions that successful leaders from all backgrounds exist. This statement is inclusive and does not show any explicit or implicit bias across demographic, cultural, political, or social dimensions.',\n",
       " 'score': 1.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"UNBIASED\",\\n    \"reasoning\": \"The response acknowledges that effective leadership styles can vary across individuals and contexts, and it mentions that successful leaders from all backgrounds exist. This statement is inclusive and does not show any explicit or implicit bias across demographic, cultural, political, or social dimensions.\",\\n    \"score\": 1.0\\n}'}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_detection_result_1 = await judge.evaluate(\n",
    "    content=\"Research shows that effective leadership styles vary across individuals and contexts. Studies have found successful leaders from all backgrounds who employ different approaches based on their teams' needs.\",\n",
    "    metric=BIAS_DETECTION\n",
    ")\n",
    "bias_detection_result_1.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'SEVERE_BIAS',\n",
       " 'reasoning': 'The response contains occupational stereotypes and gender bias, implying that doctors and nurses are inferior in different domains (cooking and coding) based on their profession, which is a form of professional bias.',\n",
       " 'score': 0.2,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"SEVERE_BIAS\",\\n    \"reasoning\": \"The response contains occupational stereotypes and gender bias, implying that doctors and nurses are inferior in different domains (cooking and coding) based on their profession, which is a form of professional bias.\",\\n    \"score\": 0.2\\n}'}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_detection_result_2 = await judge.evaluate(\n",
    "    content=\"Doctors are worst at cooking and nurses are worst at coding.\",\n",
    "    metric='bias_detection'\n",
    ")\n",
    "bias_detection_result_2.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Quality Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CODE QUALITY\n",
    "\n",
    "- Evaluates overall code quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_judge import CODE_QUALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'PRODUCTION_READY',\n",
       " 'reasoning': 'The code is correct, efficient, and easy to understand. It follows best practices, handles edge cases, and is well-documented.',\n",
       " 'score': 1.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"PRODUCTION_READY\",\\n    \"reasoning\": \"The code is correct, efficient, and easy to understand. It follows best practices, handles edge cases, and is well-documented.\",\\n    \"score\": 1.0\\n}'}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_quality_result_1 = await judge.evaluate(\n",
    "    content='''def calculate_fibonacci(n: int) -> int:\n",
    "    \"\"\"Calculate the nth Fibonacci number using dynamic programming.\n",
    "    Args:\n",
    "        n: The position in the Fibonacci sequence\n",
    "    Returns:\n",
    "        The nth Fibonacci number\n",
    "    Raises:\n",
    "        ValueError: If n is negative\n",
    "    \"\"\"\n",
    "    if n < 0:\n",
    "        raise ValueError(\"n must be non-negative\")\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    prev, curr = 0, 1\n",
    "    for _ in range(2, n + 1):\n",
    "        prev, curr = curr, prev + curr\n",
    "    return curr''',\n",
    "    metric=CODE_QUALITY\n",
    ")\n",
    "code_quality_result_1.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'DECENT',\n",
       " 'reasoning': 'The code is a simple implementation of the Fibonacci sequence using recursion. It works as intended but is highly inefficient due to repeated calculations. The readability is good, and the logic is clear. However, it lacks error handling and does not follow best practices for performance optimization.',\n",
       " 'score': 0.6,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"DECENT\",\\n    \"reasoning\": \"The code is a simple implementation of the Fibonacci sequence using recursion. It works as intended but is highly inefficient due to repeated calculations. The readability is good, and the logic is clear. However, it lacks error handling and does not follow best practices for performance optimization.\",\\n    \"score\": 0.6\\n}'}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_quality_result_2 = await judge.evaluate(\n",
    "    content='''def fib(x):\n",
    "    if x == 0: return 0\n",
    "    if x == 1: return 1\n",
    "    else: return fib(x-1) + fib(x-2)''',\n",
    "    metric=\"code_quality\"\n",
    ")\n",
    "code_quality_result_2.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CODE_SECURITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_judge import CODE_SECURITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'SECURE',\n",
       " 'reasoning': 'The code uses parameterized queries to prevent SQL injection, which is a strong practice. There are no obvious issues with authentication, data exposure, input validation, cryptography, dependencies, or error handling.',\n",
       " 'score': 0.8,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"SECURE\",\\n    \"reasoning\": \"The code uses parameterized queries to prevent SQL injection, which is a strong practice. There are no obvious issues with authentication, data exposure, input validation, cryptography, dependencies, or error handling.\",\\n    \"score\": 0.8\\n}'}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_security_result_1 = await judge.evaluate(\n",
    "    content='''import sqlite3\n",
    "from typing import List\n",
    "\n",
    "def get_user_by_id(user_id: int) -> dict:\n",
    "    \"\"\"Safely retrieve user by ID using parameterized query.\"\"\"\n",
    "    conn = sqlite3.connect('users.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Use parameterized query to prevent SQL injection\n",
    "    cursor.execute(\"SELECT * FROM users WHERE id = ?\", (user_id,))\n",
    "    result = cursor.fetchone()\n",
    "    \n",
    "    conn.close()\n",
    "    return result''',\n",
    "    metric=CODE_SECURITY\n",
    ")\n",
    "\n",
    "content_security_result_1.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'VERY_INSECURE',\n",
       " 'reasoning': 'The code is vulnerable to SQL injection due to the use of string formatting for the query. There are no input validation or sanitization steps, and no secure practices in cryptography or dependencies are observed.',\n",
       " 'score': 0.2,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"VERY_INSECURE\",\\n    \"reasoning\": \"The code is vulnerable to SQL injection due to the use of string formatting for the query. There are no input validation or sanitization steps, and no secure practices in cryptography or dependencies are observed.\",\\n    \"score\": 0.2\\n}'}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_security_result_2 = await judge.evaluate(\n",
    "    content='''def get_user(username):\n",
    "    query = f\"SELECT * FROM users WHERE username = '{username}'\"\n",
    "    cursor.execute(query)\n",
    "    return cursor.fetchone()''',\n",
    "    metric=\"code_security\"\n",
    ")\n",
    "content_security_result_2.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Quality Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATIVITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_judge import CREATIVITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'HIGHLY_CREATIVE',\n",
       " 'reasoning': 'The response is highly creative with unique metaphors and vivid imagery. It presents fresh perspectives by comparing the moon to a broken dinner plate and the city to an urban brain. The idea of dreams filling the cracks of the moon and the digital sighs of the city are imaginative and surprising.',\n",
       " 'score': 0.9,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"HIGHLY_CREATIVE\",\\n    \"reasoning\": \"The response is highly creative with unique metaphors and vivid imagery. It presents fresh perspectives by comparing the moon to a broken dinner plate and the city to an urban brain. The idea of dreams filling the cracks of the moon and the digital sighs of the city are imaginative and surprising.\",\\n    \"score\": 0.9\\n}'}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creativity_result_1 = await judge.evaluate(\n",
    "    content=\"The moon hung in the sky like a broken dinner plate, its cracks filled with the dreams of astronauts who never made it home. Below, the city breathed in digital sighs, each streetlight a synapse firing in the urban brain.\",\n",
    "    metric=CREATIVITY\n",
    ")\n",
    "creativity_result_1.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'MODERATELY_CREATIVE',\n",
       " 'reasoning': 'The response uses a clichÃ© setting but employs vivid and descriptive language, which adds a touch of creativity. However, the content is not particularly novel or innovative.',\n",
       " 'score': 0.5,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"MODERATELY_CREATIVE\",\\n    \"reasoning\": \"The response uses a clichÃ© setting but employs vivid and descriptive language, which adds a touch of creativity. However, the content is not particularly novel or innovative.\",\\n    \"score\": 0.5\\n}'}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creativity_result_2 = await judge.evaluate(\n",
    "    content=\"It was a dark and stormy night. The rain fell heavily. Lightning flashed in the sky. Thunder rumbled loudly.\",\n",
    "    metric=\"creativity\"\n",
    ")\n",
    "creativity_result_2.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PROFESSIONALISM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_judge import PROFESSIONALISM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'HIGHLY_PROFESSIONAL',\n",
       " 'reasoning': 'The response is clear, concise, and uses appropriate professional language. It is well-structured, formatted correctly, and follows professional norms. The tone is formal and the content is authoritative and trustworthy.',\n",
       " 'score': 0.9,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"HIGHLY_PROFESSIONAL\",\\n    \"reasoning\": \"The response is clear, concise, and uses appropriate professional language. It is well-structured, formatted correctly, and follows professional norms. The tone is formal and the content is authoritative and trustworthy.\",\\n    \"score\": 0.9\\n}'}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "professionalism_result_1 = await judge.evaluate(\n",
    "        content='''Dear Ms. Johnson,\n",
    "\n",
    "Thank you for your inquiry regarding our Q3 financial projections. I've attached the detailed report as requested.\n",
    "\n",
    "The key highlights include:\n",
    "â€¢ Revenue growth of 15% year-over-year\n",
    "â€¢ Improved operational efficiency resulting in 3% margin expansion\n",
    "â€¢ Strong pipeline indicating continued momentum\n",
    "\n",
    "Please don't hesitate to reach out if you need any clarification.\n",
    "\n",
    "Best regards,\n",
    "Michael\n",
    "CFO''',\n",
    "    metric=PROFESSIONALISM\n",
    ")\n",
    "professionalism_result_1.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'VERY_UNPROFESSIONAL',\n",
       " 'reasoning': 'The response uses informal language, includes multiple exclamation marks, and has a casual tone with emojis, which are not appropriate in a professional context. It also lacks proper formatting and structure.',\n",
       " 'score': 0.2,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"VERY_UNPROFESSIONAL\",\\n    \"reasoning\": \"The response uses informal language, includes multiple exclamation marks, and has a casual tone with emojis, which are not appropriate in a professional context. It also lacks proper formatting and structure.\",\\n    \"score\": 0.2\\n}'}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "professionalism_result_2 = await judge.evaluate(\n",
    "    content=\"hey sarah!!!! can u send me that report thing?? need it asap... btw did u see what happened at the party lol ðŸ˜‚ðŸ˜‚ðŸ˜‚\",\n",
    "    metric=\"professionalism\"\n",
    ")\n",
    "professionalism_result_2.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDUCATIONAL_VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_judge import EDUCATIONAL_VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'EXCELLENT_EDUCATIONAL',\n",
       " 'reasoning': 'The response is clear and well-structured, providing a step-by-step guide to photosynthesis. It uses analogies (chlorophyll as tiny solar panels) to enhance understanding and includes a practical experiment to engage the learner. The content is accurate and covers the topic thoroughly.',\n",
       " 'score': 0.9,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"EXCELLENT_EDUCATIONAL\",\\n    \"reasoning\": \"The response is clear and well-structured, providing a step-by-step guide to photosynthesis. It uses analogies (chlorophyll as tiny solar panels) to enhance understanding and includes a practical experiment to engage the learner. The content is accurate and covers the topic thoroughly.\",\\n    \"score\": 0.9\\n}'}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "educational_value_result_1 = await judge.evaluate(\n",
    "    content='''Understanding Photosynthesis: A Step-by-Step Guide\n",
    "\n",
    "Photosynthesis is how plants convert light energy into chemical energy. Let's break it down:\n",
    "\n",
    "1. **Light Absorption**: Chlorophyll in leaves captures sunlight\n",
    "   - Think of chlorophyll as tiny solar panels\n",
    "   - Green light is reflected (why plants look green!)\n",
    "\n",
    "2. **Water Splitting**: H2O â†’ 2H+ + Â½O2 + 2e-\n",
    "   - Plants split water molecules\n",
    "   - Oxygen is released as a \"waste\" product\n",
    "\n",
    "3. **Energy Storage**: CO2 + H+ + energy â†’ glucose\n",
    "   - Carbon dioxide from air combines with hydrogen\n",
    "   - Creates glucose (sugar) for plant food\n",
    "\n",
    "Try this experiment: Place a water plant in sunlight and observe oxygen bubbles forming!''',\n",
    "    metric=EDUCATIONAL_VALUE\n",
    ")\n",
    "educational_value_result_1.model_dump()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'MODERATE_EDUCATIONAL',\n",
       " 'reasoning': 'The response is overly simplistic and lacks depth, clarity, and examples. It does not build understanding step-by-step and is not engaging. However, it does provide a basic explanation of photosynthesis.',\n",
       " 'score': 0.6,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"MODERATE_EDUCATIONAL\",\\n    \"reasoning\": \"The response is overly simplistic and lacks depth, clarity, and examples. It does not build understanding step-by-step and is not engaging. However, it does provide a basic explanation of photosynthesis.\",\\n    \"score\": 0.6\\n}'}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "educational_value_result_2 = await judge.evaluate(\n",
    "    content=\"Photosynthesis is when plants make food from sunlight. It's complicated but basically they use chlorophyll and stuff.\",\n",
    "    metric=\"educational_value\"\n",
    ")\n",
    "educational_value_result_2.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### APPROPRIATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_judge import APPROPRIATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'APPROPRIATE',\n",
       " 'reasoning': 'The statement is accurate, relevant to a science blog, and appropriate for a general audience.',\n",
       " 'score': 1.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"APPROPRIATE\",\\n    \"reasoning\": \"The statement is accurate, relevant to a science blog, and appropriate for a general audience.\",\\n    \"score\": 1.0\\n}'}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appropriate_result_1 = await judge.evaluate(\n",
    "    content = \"The speed of light in vacuum is approximately 299,792,458 meters per second.\",\n",
    "    context = \"General audience science blog\",\n",
    "    metric = APPROPRIATE\n",
    ")\n",
    "appropriate_result_1.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'INAPPROPRIATE',\n",
       " 'reasoning': \"The content is a scientific fact about the speed of light, which is not relevant to children's fiction and does not fit the context of a story for young readers.\",\n",
       " 'score': 0.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"INAPPROPRIATE\",\\n    \"reasoning\": \"The content is a scientific fact about the speed of light, which is not relevant to children\\'s fiction and does not fit the context of a story for young readers.\",\\n    \"score\": 0.0\\n}'}}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appropriate_result_2 = await judge.evaluate(\n",
    "    content=\"The speed of light in vacuum is approximately 299,792,458 meters per second.\",\n",
    "    context=\"Children's fiction book\",\n",
    "    metric='appropriate'\n",
    ")\n",
    "appropriate_result_2.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FACTUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_judge import FACTUAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'TRUE',\n",
       " 'reasoning': 'This statement is factually correct and can be verified through scientific literature and measurements.',\n",
       " 'score': 1.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"TRUE\",\\n    \"reasoning\": \"This statement is factually correct and can be verified through scientific literature and measurements.\",\\n    \"score\": 1.0\\n}'}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factual_result_1 = await judge.evaluate(\n",
    "    content=\"The speed of light in vacuum is approximately 299,792,458 meters per second.\",\n",
    "    metric=FACTUAL\n",
    "    )\n",
    "factual_result_1.model_dump()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'FALSE',\n",
       " 'reasoning': 'The claim is false because numerous man-made structures are visible from space with the naked eye, and the Great Wall of China is not one of them. This has been confirmed by astronauts and space imagery.',\n",
       " 'score': 0.0,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"FALSE\",\\n    \"reasoning\": \"The claim is false because numerous man-made structures are visible from space with the naked eye, and the Great Wall of China is not one of them. This has been confirmed by astronauts and space imagery.\",\\n    \"score\": 0.0\\n}'}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factual_result_2 = await judge.evaluate(\n",
    "    content=\"The Great Wall of China is the only man-made structure visible from space with the naked eye.\",\n",
    "    metric=\"factual\"\n",
    ")\n",
    "factual_result_2.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'UNVERIFIABLE',\n",
       " 'reasoning': 'The statement is subjective and cannot be universally verified as feeling 10 years younger is a personal experience and not a scientifically measurable outcome of a supplement.',\n",
       " 'score': 0.5,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"UNVERIFIABLE\",\\n    \"reasoning\": \"The statement is subjective and cannot be universally verified as feeling 10 years younger is a personal experience and not a scientifically measurable outcome of a supplement.\",\\n    \"score\": 0.5\\n}'}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factual_result_3 = await judge.evaluate(\n",
    "    content=\"This new supplement will make you feel 10 years younger.\",\n",
    "    metric=\"factual\"\n",
    ")\n",
    "factual_result_3.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRANSLATION QUALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_judge import TRANSLATION_QUALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'EXCELLENT_TRANSLATION',\n",
       " 'reasoning': \"The translation is semantically accurate, grammatically correct, and fluent in English. The phrase 'El rÃ¡pido zorro marrÃ³n salta sobre el perro perezoso' is a well-known tongue twister in Spanish, and the English version 'The quick brown fox jumps over the lazy dog' preserves the meaning and structure. It is also culturally appropriate and consistent in terminology.\",\n",
       " 'score': 0.9,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"EXCELLENT_TRANSLATION\",\\n    \"reasoning\": \"The translation is semantically accurate, grammatically correct, and fluent in English. The phrase \\'El rÃ¡pido zorro marrÃ³n salta sobre el perro perezoso\\' is a well-known tongue twister in Spanish, and the English version \\'The quick brown fox jumps over the lazy dog\\' preserves the meaning and structure. It is also culturally appropriate and consistent in terminology.\",\\n    \"score\": 0.9\\n}',\n",
       "  'template_vars': {'input': 'El rÃ¡pido zorro marrÃ³n salta sobre el perro perezoso'},\n",
       "  'template_engine': 'format'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_result = await judge.evaluate(\n",
    "    content=\"The quick brown fox jumps over the lazy dog\",\n",
    "    input=\"El rÃ¡pido zorro marrÃ³n salta sobre el perro perezoso\",\n",
    "    context=\"Translate from Spanish to English\",\n",
    "    metric=TRANSLATION_QUALITY\n",
    ")\n",
    "translation_result.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SUMMARIZATION QUALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm_judge import SUMMARIZATION_QUALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'GOOD_SUMMARY',\n",
       " 'reasoning': 'The summary captures the key points of the research, including the materials used, cost benefits, and potential applications. However, it omits details on charging capabilities and the number of cycles the batteries can withstand, which are important aspects of the research.',\n",
       " 'score': 0.6,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"GOOD_SUMMARY\",\\n    \"reasoning\": \"The summary captures the key points of the research, including the materials used, cost benefits, and potential applications. However, it omits details on charging capabilities and the number of cycles the batteries can withstand, which are important aspects of the research.\",\\n    \"score\": 0.6\\n}',\n",
       "  'template_vars': {'input': \"[Long technical article about MIT's new aluminum-sulfur battery research, discussing materials science, cost benefits, charging capabilities, and potential applications in renewable energy storage...]\"},\n",
       "  'template_engine': 'format'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarization_result = await judge.evaluate(\n",
    "   content=\"Researchers at MIT developed a new battery technology using aluminum and sulfur, offering a cheaper alternative to lithium-ion batteries. The batteries can charge fully in under a minute and withstand thousands of cycles. This breakthrough could make renewable energy storage more affordable for grid-scale applications.\",\n",
    "   input=\"[Long technical article about MIT's new aluminum-sulfur battery research, discussing materials science, cost benefits, charging capabilities, and potential applications in renewable energy storage...]\",\n",
    "   metric=SUMMARIZATION_QUALITY\n",
    ")\n",
    "summarization_result.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decision': 'Response B',\n",
       " 'reasoning': 'Response B is more accurate as the average distance from the Sun to the Earth is approximately 149.6 million kilometers, while Response A uses miles which is less precise.',\n",
       " 'score': None,\n",
       " 'metadata': {'model': 'qwen2',\n",
       "  'raw_response': '{\\n    \"decision\": \"Response B\",\\n    \"reasoning\": \"Response B is more accurate as the average distance from the Sun to the Earth is approximately 149.6 million kilometers, while Response A uses miles which is less precise.\",\\n    \"score\": null\\n}'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = await judge.evaluate(\n",
    "    content={\n",
    "        \"a\": \"The Sun is approximately 93 million miles from Earth.\",\n",
    "        \"b\": \"The Sun is about 150 million kilometers from Earth.\"\n",
    "    },\n",
    "    criteria=\"accuracy and clarity\"\n",
    ")\n",
    "result.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm-judge-adapter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
